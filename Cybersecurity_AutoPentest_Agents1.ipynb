{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNEe0YoI4SK4y3WY1kG30Vt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumarvels/GenAIProjects/blob/main/Cybersecurity_AutoPentest_Agents1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Project Overview\n",
        "\n",
        "Cyber-AutoAgent: Agents Penetration Testing using multiple pentesting tools and LLMs, and the addition of a detailed dashboard.\n",
        "\n",
        "Why:  \n",
        "\n",
        "It explains the need for comprehensive testing, industry-standard reporting, and visual analytics through a dashboard to enhance decision-making and compliance.\n",
        "\n",
        "How: Implementation Steps\n",
        "\n",
        "The code implements the entire system, including LLM configuration, Mem0, Strands Agents SDK, pentesting tools, the CyberAutoAgent class, progress tracking, and industry-standard report automation.\n",
        "\n",
        "The dashboard is created using Streamlit and Plotly, featuring multiple interactive visualizations:\n",
        "\n",
        "Assessment Progress: Shows the current progress as a percentage.\n",
        "\n",
        "Vulnerability Distribution: A bar chart displaying vulnerabilities by risk level.\n",
        "\n",
        "Tool Usage Summary: A pie chart summarizing the tools used.\n",
        "\n",
        "Risk Trends Over Time: A line chart showing historical risk trends.\n",
        "\n",
        "Recommendations: Displays the industry-standard report content.\n",
        "\n",
        "Outcome: Expected Results\n",
        "The outcome section details the expected results, including comprehensive security assessments, standardized reports, a detailed dashboard, and continuous improvement.\n",
        "\n",
        "This single-cell notebook provides a complete, self-contained solution for DAST with a beautiful, interactive dashboard, ready for execution in Google Colab. The dashboard enhances the presentation of results, making it easier for stakeholders to understand and act on the security assessment findings.\n",
        "\n"
      ],
      "metadata": {
        "id": "DHTE-vGBadyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cybersecurity Penetration Testing Automation - Agents based Orchestration**"
      ],
      "metadata": {
        "id": "U94p598Pri9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from mem0 import Memory\n",
        "from typing import Dict, Any\n",
        "!pip install streamlit plotly pandas genai\n",
        "\n",
        "# Conditional import for nmap, ipywidgets, and plotly/streamlit for Colab/Dashboard\n",
        "try:\n",
        "    import nmap\n",
        "except ImportError:\n",
        "    print(\"python-nmap not found. Please install it: pip install python-nmap\")\n",
        "    nmap = None\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    print(\"ipywidgets not found. Progress bar will be printed to console.\")\n",
        "    widgets = None\n",
        "    display = None\n",
        "\n",
        "\n",
        "## Section: LLM Manager\n",
        "# What: The LLMManager class is responsible for managing different Large Language Models (LLMs).\n",
        "# Why: It provides a centralized way to add, configure, and retrieve various LLMs (like Gemini, OpenAI, etc.). This makes the code more modular and flexible, allowing easy switching or adding new LLMs without modifying core logic.\n",
        "# How: It uses a dictionary to store LLM configurations, each with a name (e.g., \"gemini\") and a dictionary containing the LLM client and a configuration function.\n",
        "# Outcomes: A streamlined way to handle multiple LLM integrations, improving maintainability and extensibility.\n",
        "class LLMManager:\n",
        "    def __init__(self):\n",
        "        self.llms = {}\n",
        "\n",
        "    def add_llm(self, name: str, config: Dict[str, Any]):\n",
        "        self.llms[name] = config\n",
        "\n",
        "    def get_llm(self, name: str):\n",
        "        if name not in self.llms:\n",
        "            raise ValueError(f\"LLM {name} not configured.\")\n",
        "        return self.llms[name]\n",
        "\n",
        "llm_manager = LLMManager()\n",
        "# Assuming GOOGLE_API_KEY is set in environment variables or Colab secrets\n",
        "llm_manager.add_llm(\"gemini\", {\"client\": genai.GenerativeModel(\"gemini-pro\"), \"configure\": lambda: genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))})\n",
        "\n",
        "try:\n",
        "    from azure.ai.textanalytics import TextAnalyticsClient\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    llm_manager.add_llm(\"azure_ai\", {\"client\": TextAnalyticsClient(endpoint=\"https://your-azure-endpoint.cognitiveservices.azure.com/\", credential=AzureKeyCredential(os.environ[\"AZURE_AI_KEY\"])), \"configure\": lambda: None})\n",
        "except ImportError:\n",
        "    print(\"Azure Text Analytics modules not found. Skipping Azure AI LLM configuration.\")\n",
        "except KeyError:\n",
        "    print(\"AZURE_AI_KEY environment variable not set. Skipping Azure AI LLM configuration.\")\n",
        "\n",
        "\n",
        "## Section: Mem0 Configuration\n",
        "# What: This segment initializes the Mem0 memory client, which is used for persistent storage and retrieval of assessment data.\n",
        "# Why: Memory is crucial for an agent to learn from past interactions, store assessment results, and provide context for future actions or reports. It prevents redundant work and enables more intelligent decision-making.\n",
        "# How: It attempts to initialize `mem0.Memory` by passing the `gemini-pro` LLM client. It includes error handling for cases where the LLM configuration or Mem0 initialization fails.\n",
        "# Outcomes: The `memory` object is available for the CyberAgent to store and retrieve DAST findings, enabling statefulness and context-awareness.\n",
        "try:\n",
        "    gemini_client_for_memory = llm_manager.get_llm(\"gemini\")[\"client\"]\n",
        "    try:\n",
        "        memory = Memory(llm=gemini_client_for_memory)\n",
        "    except TypeError:\n",
        "        print(\"Attempted to initialize Mem0 with 'llm' parameter, failed. Trying 'client'.\")\n",
        "        try:\n",
        "            memory = Memory(client=gemini_client_for_memory)\n",
        "        except TypeError:\n",
        "            print(\"Error: Mem0 initialization failed with both 'llm' and 'client' arguments. Check Mem0 documentation.\")\n",
        "            memory = None\n",
        "except ValueError as e:\n",
        "    print(f\"Error configuring Mem0: {e}. Make sure Gemini API key is set.\")\n",
        "    memory = None\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Mem0: {e}. Check Mem0 documentation for correct initialization.\")\n",
        "    memory = None\n",
        "\n",
        "\n",
        "## Section: Tool Assessment Functions\n",
        "# What: These are individual Python functions, each representing a specific cybersecurity assessment tool (e.g., ZAP, SQLMap, Nikto, Nmap).\n",
        "# Why: They encapsulate the logic for executing external command-line tools or interacting with APIs of security assessment platforms. This modularity allows the agent to call diverse tools based on the assessment needs.\n",
        "# How: Each function typically uses `subprocess.run` to execute a command-line tool with specific arguments or contains placeholder logic for API interactions. They capture the output and return it. Error handling is included for missing tools or execution failures.\n",
        "# Outcomes: The agent can leverage a variety of DAST tools programmatically, widening its assessment capabilities. Each function provides a standardized interface for tool execution.\n",
        "def run_zap_assessment(url):\n",
        "    # What: Placeholder for ZAP (OWASP ZAP) assessment.\n",
        "    # Why: ZAP is a popular open-source DAST tool for finding vulnerabilities in web applications.\n",
        "    # How: In a real scenario, this would involve interacting with ZAP's API to initiate spidering, active scans, and retrieve alerts. Currently, it simulates results.\n",
        "    # Outcomes: (Simulated) DAST findings like SQL Injection and Cross-Site Scripting.\n",
        "    try:\n",
        "        print(f\"Running ZAP assessment on {url}\")\n",
        "        alerts = [{'alert': 'SQL Injection', 'risk': 'High', 'url': url},\n",
        "                  {'alert': 'Cross-Site Scripting', 'risk': 'Medium', 'url': url}]\n",
        "        return alerts\n",
        "    except Exception as e:\n",
        "        print(f\"Error running ZAP assessment: {e}\")\n",
        "        return f\"ZAP assessment failed: {e}\"\n",
        "\n",
        "def run_burp_assessment(url):\n",
        "    # What: Placeholder for Burp Suite assessment.\n",
        "    # Why: Burp Suite is a leading integrated platform for web application security testing.\n",
        "    # How: In a real scenario, this would involve Burp's API for automated scans or integrating with its proxy for manual testing data.\n",
        "    # Outcomes: (Simulated) An indication that a Burp Suite assessment was performed.\n",
        "    print(f\"Running Burp Suite assessment on {url}\")\n",
        "    return \"Burp Suite assessment simulated.\"\n",
        "\n",
        "def run_sqlmap_assessment(url):\n",
        "    # What: Executes SQLMap for SQL Injection vulnerability detection.\n",
        "    # Why: SQLMap is an open-source tool for automating the process of detecting and exploiting SQL injection flaws.\n",
        "    # How: Uses `subprocess.run` to call the `sqlmap` command with the target URL.\n",
        "    # Outcomes: Output from SQLMap detailing potential SQL injection vulnerabilities.\n",
        "    try:\n",
        "        result = subprocess.run(['sqlmap', '-u', url, '--batch'], capture_output=True, text=True, check=True)\n",
        "        return result.stdout\n",
        "    except FileNotFoundError:\n",
        "        return \"sqlmap not found. Please install it and ensure it's in your PATH.\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"sqlmap assessment failed: {e.stderr}\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_nikto_assessment(url):\n",
        "    # What: Executes Nikto for web server vulnerability scanning.\n",
        "    # Why: Nikto is a web server scanner that tests web servers for dangerous files/CGIs, outdated server software, and other problems.\n",
        "    # How: Uses `subprocess.run` to call the `nikto` command with the target URL.\n",
        "    # Outcomes: Output from Nikto detailing web server configuration issues and potential vulnerabilities.\n",
        "    try:\n",
        "        result = subprocess.run(['nikto', '-h', url], capture_output=True, text=True, check=True)\n",
        "        return result.stdout\n",
        "    except FileNotFoundError:\n",
        "        return \"nikto not found. Please install it and ensure it's in your PATH.\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"nikto assessment failed: {e.stderr}\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_nessus_assessment(url):\n",
        "    # What: Placeholder for Nessus vulnerability scanning.\n",
        "    # Why: Nessus is a widely used vulnerability scanner, often for network and system-level vulnerabilities, but can include web applications.\n",
        "    # How: In a real implementation, this would involve Nessus API calls to initiate scans and retrieve reports.\n",
        "    # Outcomes: (Simulated) An indication that a Nessus assessment was performed.\n",
        "    try:\n",
        "        print(f\"Running Nessus assessment on {url}\")\n",
        "        return \"Nessus assessment simulated.\"\n",
        "    except FileNotFoundError:\n",
        "        return \"Nessus not found. Please install it.\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_nmap_assessment(url):\n",
        "    # What: Executes Nmap for network discovery and security auditing.\n",
        "    # Why: Nmap is an essential tool for network reconnaissance, identifying open ports, services, and operating systems.\n",
        "    # How: Uses the `python-nmap` library to perform a service version detection scan (`-sV`) on the target URL's host.\n",
        "    # Outcomes: Network and service information (e.g., open ports, running services) in CSV format.\n",
        "    try:\n",
        "        if nmap:\n",
        "            nm = nmap.PortScanner()\n",
        "            nm.scan(url, arguments='-sV')\n",
        "            return nm.csv()\n",
        "        else:\n",
        "            return \"nmap (python-nmap) module not available.\"\n",
        "    except FileNotFoundError:\n",
        "        return \"nmap executable not found. Please install it and ensure it's in your PATH.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error running Nmap assessment: {e}\")\n",
        "        return f\"nmap assessment failed: {e}\"\n",
        "\n",
        "def run_metastalker_assessment(url):\n",
        "    # What: Executes Metastalker for metadata extraction from publicly available documents.\n",
        "    # Why: Metadata can sometimes reveal sensitive information about an organization or its infrastructure.\n",
        "    # How: Uses `subprocess.run` to call the `metastalker` command with the target URL.\n",
        "    # Outcomes: Extracted metadata, which could include author names, software versions, or creation dates.\n",
        "    try:\n",
        "        result = subprocess.run(['metastalker', '-u', url], capture_output=True, text=True, check=True)\n",
        "        return result.stdout\n",
        "    except FileNotFoundError:\n",
        "        return \"metastalker not found. Please install it and ensure it's in your PATH.\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"metastalker assessment failed: {e.stderr}\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_wpscan_assessment(url):\n",
        "    # What: Executes WPScan for WordPress vulnerability scanning.\n",
        "    # Why: WPScan is a specialized tool for identifying vulnerabilities in WordPress installations, including themes, plugins, and core.\n",
        "    # How: Uses `subprocess.run` to call the `wpscan` command, formatted for JSON output.\n",
        "    # Outcomes: JSON output containing detected vulnerabilities, theme/plugin issues, and user enumeration results for WordPress sites.\n",
        "    try:\n",
        "        result = subprocess.run(['wpscan', '--url', url, '--batch', '--format', 'json'], capture_output=True, text=True, check=True)\n",
        "        import json\n",
        "        try:\n",
        "            return json.loads(result.stdout)\n",
        "        except json.JSONDecodeError:\n",
        "            return result.stdout\n",
        "    except FileNotFoundError:\n",
        "        return \"wpscan not found. Please install it and ensure it's in your PATH.\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"wpscan assessment failed: {e.stderr}\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_dirb_assessment(url):\n",
        "    # What: Executes Dirb for web content scanning (bruteforcing directories and files).\n",
        "    # Why: Dirb is used to find hidden web objects (directories, files, etc.) that might not be linked from the main site.\n",
        "    # How: Uses `subprocess.run` to call the `dirb` command, outputting results to a temporary file.\n",
        "    # Outcomes: A list of discovered directories and files, which could indicate misconfigurations or sensitive data.\n",
        "    try:\n",
        "        result = subprocess.run(['dirb', url, '-o', '/tmp/dirb_output.txt'], capture_output=True, text=True)\n",
        "        with open('/tmp/dirb_output.txt', 'r') as f:\n",
        "            output = f.read()\n",
        "        return output\n",
        "    except FileNotFoundError:\n",
        "        return \"dirb not found. Please install it and ensure it's in your PATH.\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def run_arachni_assessment(url):\n",
        "    # What: Executes Arachni for web application security scanning.\n",
        "    # Why: Arachni is a feature-packed Ruby framework designed to help penetration testers and administrators evaluate the security of web applications.\n",
        "    # How: Uses `subprocess.run` to call the `arachni` command, formatted for JSON output.\n",
        "    # Outcomes: JSON output detailing various web application vulnerabilities detected by Arachni.\n",
        "    try:\n",
        "        result = subprocess.run(['arachni', '--url', url, '--output-format', 'json'], capture_output=True, text=True, check=True)\n",
        "        import json\n",
        "        try:\n",
        "            return json.loads(result.stdout)\n",
        "        except json.JSONDecodeError:\n",
        "            return result.stdout\n",
        "    except FileNotFoundError:\n",
        "        return \"arachni not found. Please install it and ensure it's in your PATH.\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"arachni assessment failed: {e.stderr}\"\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "\n",
        "## Section: ToolManager Class\n",
        "# What: The ToolManager class centralizes the management and execution of various cybersecurity assessment tools.\n",
        "# Why: It provides a single interface to access and run all defined assessment tools. This abstracts the complexity of calling different tools and ensures consistency. It also includes checks for tool availability.\n",
        "# How: It stores a dictionary of tool names mapped to their respective functions. The `run_tool` method verifies if a tool is present and executable before attempting to run it, then calls the corresponding function.\n",
        "# Outcomes: A robust and extensible system for orchestrating DAST tool execution, with built-in checks for tool readiness.\n",
        "class ToolManager:\n",
        "    def __init__(self):\n",
        "        self.tools = {\n",
        "            \"zap\": run_zap_assessment,\n",
        "            \"burp\": run_burp_assessment,\n",
        "            \"sqlmap\": run_sqlmap_assessment,\n",
        "            \"nikto\": run_nikto_assessment,\n",
        "            \"nessus\": run_nessus_assessment,\n",
        "            \"nmap\": run_nmap_assessment,\n",
        "            \"metastalker\": run_metastalker_assessment,\n",
        "            \"wpscan\": run_wpscan_assessment,\n",
        "            \"dirb\": run_dirb_assessment,\n",
        "            \"arachni\": run_arachni_assessment\n",
        "        }\n",
        "\n",
        "    def add_tool(self, name, function):\n",
        "        self.tools[name] = function\n",
        "\n",
        "    def run_tool(self, name, url):\n",
        "        if name in self.tools:\n",
        "            try:\n",
        "                # Basic check for some common tools\n",
        "                if name in [\"sqlmap\", \"nikto\", \"metastalker\", \"wpscan\", \"dirb\", \"arachni\"]:\n",
        "                    subprocess.run([name, '--version'], capture_output=True, text=True, check=True)\n",
        "                elif name == \"nmap\":\n",
        "                    subprocess.run(['nmap', '--version'], capture_output=True, text=True, check=True)\n",
        "                return self.tools[name](url)\n",
        "            except (FileNotFoundError, subprocess.CalledProcessError):\n",
        "                print(f\"Warning: Tool '{name}' not found or not executable. Skipping assessment with this tool.\")\n",
        "                return f\"Tool '{name}' not found or not executable.\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error running tool '{name}': {e}\")\n",
        "                return str(e)\n",
        "        return \"Tool not found.\"\n",
        "\n",
        "tool_manager = ToolManager()\n",
        "\n",
        "## Section: GeminiCyberAgent Class\n",
        "# What: The core class of the cybersecurity agent, orchestrating LLM interactions, tool execution, and report generation.\n",
        "# Why: This class brings together all components (LLM, ToolManager, Memory) to perform comprehensive DAST assessments, analyze results, and generate human-readable reports.\n",
        "# How: It initializes with an LLM client, a ToolManager instance, and a memory client. It contains methods for performing DAST, generating reports, and providing analytics.\n",
        "# Outcomes: A functional and intelligent agent capable of autonomous security assessments and reporting.\n",
        "class GeminiCyberAgent:\n",
        "    def __init__(self, llm_client, tool_manager, memory=None):\n",
        "        # What: Constructor for the GeminiCyberAgent.\n",
        "        # Why: Initializes the agent with necessary components for its operations.\n",
        "        # How: Takes an LLM client, a tool manager, and an optional memory instance as arguments and assigns them to instance variables. Also initializes progress tracking.\n",
        "        # Outcomes: A ready-to-use CyberAgent instance.\n",
        "        self.llm_client = llm_client\n",
        "        self.tool_manager = tool_manager\n",
        "        self.memory = memory\n",
        "        self.progress = 0\n",
        "        self.total_tasks = 0\n",
        "\n",
        "    def perform_dast(self, url):\n",
        "        # What: Performs a Dynamic Application Security Test (DAST) on a given URL.\n",
        "        # Why: This is the primary function for initiating a security assessment. It intelligently selects the best tool using the LLM and then executes it.\n",
        "        # How: It constructs a prompt for the LLM to choose the most appropriate tool from the available list, considering past assessments. It then runs the chosen tool and stores the results in memory.\n",
        "        # Outcomes: DAST findings from the selected tool, stored in memory for later reporting and analysis.\n",
        "        if not self.llm_client:\n",
        "            print(\"LLM client not initialized.\")\n",
        "            return \"Assessment skipped.\"\n",
        "\n",
        "        self.total_tasks += 1\n",
        "\n",
        "        past_assessments_context = \"\"\n",
        "        if self.memory:\n",
        "            past_assessments = self.memory.search(\"DAST for \" + url, user_id=\"guest\")\n",
        "            if past_assessments:\n",
        "                past_assessments_context = \"Previous DAST findings: \" + str(past_assessments)\n",
        "\n",
        "        available_tools = list(self.tool_manager.tools.keys())\n",
        "        prompt = f\"\"\"You are an expert in web application security testing. Your task is to select the best tool for performing a dynamic application security test (DAST) on the following URL: {url}.\n",
        "Available tools: {', '.join(available_tools)}.\n",
        "{past_assessments_context}\n",
        "Based on the URL and any previous findings, which tool would be the most effective? Respond with only the name of the chosen tool from the available tools list (e.g., \"zap\", \"sqlmap\", \"nmap\").\n",
        "\"\"\"\n",
        "        try:\n",
        "            safety_settings = [\n",
        "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            ]\n",
        "            response = self.llm_client.generate_content(prompt, safety_settings=safety_settings)\n",
        "            tool_choice = response.text.strip().lower()\n",
        "\n",
        "            if tool_choice not in available_tools:\n",
        "                print(f\"LLM chose an invalid tool: {tool_choice}. Defaulting to 'zap'.\")\n",
        "                tool_choice = \"zap\"\n",
        "\n",
        "            print(f\"LLM chose tool: {tool_choice}\")\n",
        "\n",
        "            results = self.tool_manager.run_tool(tool_choice, url)\n",
        "\n",
        "            if self.memory:\n",
        "                self.memory.add(f\"DAST results for {url} using {tool_choice}: {str(results)}\")\n",
        "\n",
        "            self.update_progress()\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during LLM interaction or tool execution: {e}\")\n",
        "            return f\"Assessment failed: {e}\"\n",
        "\n",
        "\n",
        "    def generate_report(self, url):\n",
        "        # What: Generates a general security assessment report.\n",
        "        # Why: To summarize the findings from the DAST assessment in a human-readable format.\n",
        "        # How: Retrieves stored DAST assessments from memory, then uses the LLM to synthesize this information into a cohesive report, including an executive summary, findings, and recommendations.\n",
        "        # Outcomes: A text-based security report summarizing the assessment results.\n",
        "        if self.memory:\n",
        "            assessments = self.memory.search(\"DAST for \" + url, user_id=\"guest\")\n",
        "            print(f\"Generating report based on: {assessments}\")\n",
        "            if self.llm_client:\n",
        "                report_prompt = f\"Generate a security assessment report for {url} based on the following findings: {str(assessments)}. Include an executive summary, detailed findings by risk level, and recommendations.\"\n",
        "                try:\n",
        "                    report_response = self.llm_client.generate_content(report_prompt)\n",
        "                    return report_response.text\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating report with LLM: {e}\")\n",
        "                    return \"Report generation failed.\"\n",
        "            else:\n",
        "                return \"LLM client not available for report generation.\"\n",
        "        else:\n",
        "            print(\"Memory not available, cannot generate report.\")\n",
        "            return \"Report generation skipped. Memory not available.\"\n",
        "\n",
        "    def generate_analytics(self, url):\n",
        "        # What: Generates analytical insights from the assessment data.\n",
        "        # Why: To provide a structured, quantifiable overview of vulnerabilities, useful for dashboards and trend analysis.\n",
        "        # How: Fetches assessment data from memory, filters for structured (dictionary) findings, and converts them into a Pandas DataFrame. It specifically looks for a 'risk' column to categorize vulnerabilities.\n",
        "        # Outcomes: A Pandas DataFrame containing structured assessment data, suitable for further analysis and visualization (e.g., vulnerability distribution charts).\n",
        "        if self.memory:\n",
        "            assessments = self.memory.search(\"DAST for \" + url, user_id=\"guest\")\n",
        "            try:\n",
        "                valid_assessments = [a for a in assessments if isinstance(a, dict)]\n",
        "                if valid_assessments:\n",
        "                    df = pd.DataFrame(valid_assessments)\n",
        "                    if 'risk' in df.columns:\n",
        "                        df['risk'] = df['risk'].str.capitalize()\n",
        "                        return df\n",
        "                    else:\n",
        "                        print(\"Warning: 'risk' column not found in assessment data. Cannot generate vulnerability distribution.\")\n",
        "                        return pd.DataFrame(valid_assessments)\n",
        "                else:\n",
        "                    print(\"No valid assessment data (dictionary format) to generate analytics.\")\n",
        "                    return pd.DataFrame()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating DataFrame from assessments: {e}\")\n",
        "                return pd.DataFrame()\n",
        "        else:\n",
        "            print(\"Memory not available, cannot generate analytics.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "    def generate_industry_standard_report(self, url):\n",
        "        # What: Generates a comprehensive, industry-standard security assessment report.\n",
        "        # Why: Provides a more formal and detailed report suitable for various stakeholders, aligning with typical cybersecurity report structures.\n",
        "        # How: Retrieves structured assessment data from memory. It then prompts the LLM to generate content for predefined sections (Executive Summary, Methodology, Findings by Risk, Recommendations). It includes a fallback template if the LLM's response isn't perfectly structured.\n",
        "        # Outcomes: A detailed Markdown-formatted report, organized into standard sections, ideal for official documentation.\n",
        "        if not self.llm_client:\n",
        "            return \"Industry standard report generation skipped. LLM client not initialized.\"\n",
        "\n",
        "        assessments = self.memory.search(\"DAST for \" + url, user_id=\"guest\") if self.memory else []\n",
        "        valid_assessments = [a for a in assessments if isinstance(a, dict)]\n",
        "\n",
        "        if not valid_assessments and self.memory:\n",
        "            print(\"No valid assessment data (dictionary format) found in memory for industry report.\")\n",
        "            return f\"# Security Assessment Report for {url}\\n\\n## Executive Summary\\nNo structured vulnerability data found in memory.\\n\\n## Findings\\nNo structured findings available.\\n\\n## Recommendations\\nNo specific recommendations based on structured data.\\n\\nPrepared by: CyberAutoAgent\\nDate: {time.strftime('%Y-%m-%d')}\"\n",
        "        elif not self.memory:\n",
        "            return \"Industry standard report generation skipped. Memory not available.\"\n",
        "\n",
        "        try:\n",
        "            report_content_prompt = f\"\"\"Generate content for a security assessment report based on the following findings for {url}: {str(valid_assessments)}.\n",
        "            Provide the following sections:\n",
        "            1.  Executive Summary: A brief overview of the assessment and key findings.\n",
        "            2.  Methodology: Mention the types of tools used (e.g., DAST tools).\n",
        "            3.  Findings (High-Risk): Detail high-risk vulnerabilities.\n",
        "            4.  Findings (Medium-Risk): Detail medium-risk vulnerabilities.\n",
        "            5.  Findings (Low-Risk): Detail low-risk vulnerabilities.\n",
        "            6.  Recommendations: Actionable steps to mitigate identified vulnerabilities.\n",
        "            Format the response clearly with section headers.\"\"\"\n",
        "\n",
        "            safety_settings = [\n",
        "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "            ]\n",
        "            report_response = self.llm_client.generate_content(report_content_prompt, safety_settings=safety_settings)\n",
        "            report_content = report_response.text\n",
        "\n",
        "            sections = report_content.split(\"## \")\n",
        "            report_dict = {}\n",
        "            for section in sections:\n",
        "                if section.strip():\n",
        "                    lines = section.split('\\n', 1)\n",
        "                    if len(lines) > 1:\n",
        "                        header = lines[0].strip()\n",
        "                        content = lines[1].strip()\n",
        "                        report_dict[header] = content\n",
        "\n",
        "            if not report_dict:\n",
        "                print(\"Warning: LLM report generation did not produce expected sections. Falling back to template.\")\n",
        "                high_risk = [str(a) for a in valid_assessments if a.get('risk', '').lower() == 'high']\n",
        "                medium_risk = [str(a) for a in valid_assessments if a.get('risk', '').lower() == 'medium']\n",
        "                low_risk = [str(a) for a in valid_assessments if a.get('risk', '').lower() == 'low']\n",
        "\n",
        "                recommendations_llm = \"\"\n",
        "                if self.llm_client:\n",
        "                    try:\n",
        "                        rec_prompt = \"Provide concise recommendations to mitigate the following web vulnerabilities: \" + str(valid_assessments)\n",
        "                        rec_response = self.llm_client.generate_content(rec_prompt)\n",
        "                        recommendations_llm = rec_response.text\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error generating recommendations with LLM: {e}\")\n",
        "                        recommendations_llm = \"Recommendations could not be generated.\"\n",
        "\n",
        "                report_template = \"\"\"\n",
        "# Security Assessment Report for {url}\n",
        "\n",
        "## Executive Summary\n",
        "{executive_summary}\n",
        "\n",
        "## Methodology\n",
        "The assessment utilized dynamic analysis and tools such as {tools}.\n",
        "\n",
        "## Findings\n",
        "### High-Risk Vulnerabilities\n",
        "{high_risk}\n",
        "\n",
        "### Medium-Risk Vulnerabilities\n",
        "{medium_risk}\n",
        "\n",
        "### Low-Risk Vulnerabilities\n",
        "{low_risk}\n",
        "\n",
        "## Recommendations\n",
        "{recommendations}\n",
        "\n",
        "## Conclusion\n",
        "The assessment identified {total_vulnerabilities} vulnerabilities.\n",
        "\n",
        "Prepared by: CyberAutoAgent\n",
        "Date: {date}\n",
        "\"\"\"\n",
        "                return report_template.format(\n",
        "                    url=url,\n",
        "                    executive_summary=report_dict.get('Executive Summary', 'No executive summary generated.'),\n",
        "                    tools=', '.join(tool_manager.tools.keys()),\n",
        "                    high_risk='\\n'.join(high_risk) if high_risk else 'None identified.',\n",
        "                    medium_risk='\\n'.join(medium_risk) if medium_risk else 'None identified.',\n",
        "                    low_risk='\\n'.join(low_risk) if low_risk else 'None identified.',\n",
        "                    recommendations=report_dict.get('Recommendations', recommendations_llm if recommendations_llm else 'No recommendations generated.'),\n",
        "                    total_vulnerabilities=len(valid_assessments),\n",
        "                    date=time.strftime(\"%Y-%m-%d\")\n",
        "                )\n",
        "            return report_content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during report generation with LLM: {e}\")\n",
        "            return f\"Report generation failed: {e}\"\n",
        "\n",
        "    def update_progress(self):\n",
        "        # What: Updates the progress of the assessment.\n",
        "        # Why: Provides real-time feedback to the user on the agent's activity, especially useful in long-running tasks.\n",
        "        # How: Increments a counter for completed tasks. If `ipywidgets` is available (as in Colab), it updates a graphical progress bar; otherwise, it prints to the console.\n",
        "        # Outcomes: Improved user experience through visible progress tracking.\n",
        "        self.progress += 1\n",
        "        try:\n",
        "            if 'ipywidgets' in sys.modules and 'progress_bar' in globals():\n",
        "                progress_value = min(int((self.progress / self.total_tasks) * 100), 100)\n",
        "                progress_bar.value = progress_value\n",
        "                progress_bar.description = f'Progress: {self.progress}/{self.total_tasks} tasks completed'\n",
        "        except Exception as e:\n",
        "            print(f\"Error updating progress bar: {e}\")\n",
        "\n",
        "        if 'ipywidgets' not in sys.modules or 'progress_bar' not in globals():\n",
        "            if self.total_tasks > 0:\n",
        "                print(f\"Progress: {self.progress}/{self.total_tasks} tasks completed\")\n",
        "\n",
        "\n",
        "## Section: Agent Initialization and Example Usage\n",
        "# What: This section initializes the `GeminiCyberAgent` and demonstrates how to use its core functionalities.\n",
        "# Why: It serves as the entry point for running the security assessment process and showcasing the agent's capabilities from DAST to report generation.\n",
        "# How: It retrieves the configured Gemini LLM client, instantiates the `GeminiCyberAgent` with the LLM, ToolManager, and Memory. Then, it calls `perform_dast`, `generate_report`, `generate_industry_standard_report`, and `generate_analytics` methods for a specified URL.\n",
        "# Outcomes: Execution of a full DAST workflow, resulting in printed assessment results, various reports, and analytical data.\n",
        "try:\n",
        "    gemini_llm_client = llm_manager.get_llm(\"gemini\")[\"client\"]\n",
        "    cyber_agent = GeminiCyberAgent(llm_client=gemini_llm_client, tool_manager=tool_manager, memory=memory)\n",
        "except ValueError as e:\n",
        "    print(f\"Error initializing GeminiCyberAgent: {e}. Make sure Gemini API key is set and LLM is configured.\")\n",
        "    cyber_agent = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during GeminiCyberAgent initialization: {e}\")\n",
        "    cyber_agent = None\n",
        "\n",
        "\n",
        "url = \"http://example.com\"\n",
        "if cyber_agent:\n",
        "    cyber_agent.total_tasks = 1\n",
        "\n",
        "    print(f\"Starting DAST on {url}...\")\n",
        "    result = cyber_agent.perform_dast(url)\n",
        "    print(\"\\nDAST Result:\")\n",
        "    print(result)\n",
        "\n",
        "    print(\"\\nGenerating Report...\")\n",
        "    report = cyber_agent.generate_report(url)\n",
        "    print(\"\\nGenerated Report:\")\n",
        "    print(report)\n",
        "\n",
        "    print(\"\\nGenerating Industry Standard Report...\")\n",
        "    industry_report = cyber_agent.generate_industry_standard_report(url)\n",
        "    print(\"\\nGenerated Industry Standard Report:\")\n",
        "    print(industry_report)\n",
        "\n",
        "    print(\"\\nGenerating Analytics...\")\n",
        "    analytics = cyber_agent.generate_analytics(url)\n",
        "    print(\"\\nGenerated Analytics (DataFrame head):\")\n",
        "    if not analytics.empty:\n",
        "        display(analytics.head())\n",
        "    else:\n",
        "        print(\"No analytics data to display.\")\n",
        "\n",
        "\n",
        "## Section: Progress Bar Display\n",
        "# What: Initializes and displays an `ipywidgets` progress bar in environments like Google Colab.\n",
        "# Why: To provide a visual indicator of the assessment progress, enhancing the user experience during potentially long-running operations.\n",
        "# How: It checks for the availability of the `ipywidgets` module and, if present, creates an `IntProgress` widget and displays it. The `cyber_agent.update_progress()` method is responsible for updating this bar.\n",
        "# Outcomes: A dynamic progress bar that visually tracks the completion of assessment tasks.\n",
        "if 'ipywidgets' in sys.modules and 'progress_bar' not in globals():\n",
        "    progress_bar = widgets.IntProgress(value=0, min=0, max=100, description='Progress:', bar_style='info', orientation='horizontal')\n",
        "    display(progress_bar)\n",
        "elif 'ipywidgets' not in sys.modules:\n",
        "    print(\"ipywidgets not available, skipping progress bar display.\")\n",
        "\n",
        "\n",
        "## Section: Save Industry Standard Report\n",
        "# What: Saves the generated industry-standard report to a Markdown file.\n",
        "# Why: To persist the comprehensive report, making it available for offline viewing, sharing, or archiving.\n",
        "# How: Checks if the `cyber_agent` is initialized and if an `industry_report` has been generated. If so, it creates a unique filename and writes the report content to a `.md` file.\n",
        "# Outcomes: A physical file containing the detailed security assessment report, ready for external use.\n",
        "if cyber_agent and industry_report:\n",
        "    try:\n",
        "        report_filename = f\"security_report_{url.replace('http://', '').replace('https://', '').replace('/', '_')}_{time.strftime('%Y%m%d_%H%M%S')}.md\"\n",
        "        with open(report_filename, \"w\") as f:\n",
        "            f.write(industry_report)\n",
        "        print(f\"Industry-standard report generated and saved as {report_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving industry standard report: {e}\")\n",
        "else:\n",
        "    print(\"Industry-standard report not saved because CyberAgent was not initialized or report was not generated.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "92uAOWC0ro5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Streamlit Dashboard (Currently Commented Out)**\n",
        "\n",
        "What:\n",
        "\n",
        "The commented-out code block is designed to create a web-based Streamlit dashboard for visualizing the security assessment data.\n",
        "\n",
        "Why:\n",
        "\n",
        "A dashboard provides a much more intuitive and interactive way to understand assessment results compared to raw text. It allows for quick insights into vulnerability distribution, tool usage, and potential risk trends.\n",
        "\n",
        "How:\n",
        "\n",
        "Streamlit Installation: Streamlit is a Python library for building interactive web applications. It needs to be installed separately (pip install streamlit plotly pandas).\n",
        "Dashboard Components: The code uses Streamlit functions (st.title, st.header, st.plotly_chart, st.markdown) to lay out the dashboard.\n",
        "\n",
        "Data Visualization:\n",
        "\n",
        "It leverages Plotly Express (px) to create interactive charts, such as a bar chart for vulnerability distribution by risk level and a line chart for simulated risk trends over time.\n",
        "\n",
        "Integration with Agent:\n",
        "\n",
        "It calls the cyber_agent's generate_analytics and generate_industry_standard_report methods to fetch data directly for display.\n",
        "\n",
        "Outcomes:\n",
        "\n",
        "An interactive web dashboard that presents security assessment findings visually.\n",
        "\n",
        "Clearer understanding of the types and severity of vulnerabilities.\n",
        "\n",
        "Insights into tool usage and potential risk trends.\n",
        "A professional interface for sharing assessment summaries.\n",
        "\n",
        "How to Run the Streamlit Dashboard\n",
        "\n",
        "Since Google Colab environments are designed for notebook execution, running a full-fledged Streamlit app directly within a single cell can be tricky. Typically, you'd save the Streamlit code to a .py file and then run it from your terminal.\n",
        "\n",
        "Steps to run the Streamlit Dashboard (outside of a direct Colab cell):\n",
        "\n",
        "Install necessary libraries:\n",
        "\n",
        "Bash\n",
        "\n",
        "!pip install streamlit plotly pandas\n",
        "(Note: pandas is usually pre-installed in Colab)\n",
        "\n",
        "Uncomment the Streamlit code: In the provided Python code, remove the # comments from the entire Streamlit dashboard block.\n",
        "\n",
        "Save the code to a Python file: Copy the entire Python code (with the Streamlit block uncommented) into a new file named, for example, cyber_dashboard.py. You can do this in Colab by going to File > New file and pasting the content.\n",
        "\n",
        "Run the Streamlit app: In a new Colab code cell, or from your terminal if you download the file, execute:\n",
        "\n",
        "in shell/powershell : Bash\n",
        "\n",
        "!streamlit run cyber_dashboard.py & npx localtunnel --port 8501\n",
        "\n",
        "This command starts the Streamlit app and then uses localtunnel to create a public URL so you can access the dashboard from your browser. Colab often provides a public IP directly, but localtunnel is a reliable fallback.\n",
        "\n",
        "After running, localtunnel will provide you with a public URL (e.g., https://xxxx-xxxx-xxxx.loca.lt) which you can open in your web browser to view the interactive dashboard."
      ],
      "metadata": {
        "id": "pAEXNKDutMQ8"
      }
    }
  ]
}
