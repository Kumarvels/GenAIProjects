{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNJKlN1zrFbQukm/1A53Phu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kumarvels/GenAIProjects/blob/main/N8N_Workflow_to_Scrap_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T9n_JehcCDV"
      },
      "outputs": [],
      "source": [
        "# @title Complete Workflow: Twitter Scraping with MCP and Agno Integration\n",
        "##\n",
        "# What\n",
        "## Overview\n",
        "# We are building a workflow that automates the process of scraping Twitter (X) # for top-performing posts based on a keyword, integrates with an MCP (Message # Control Protocol) server for decentralized communication, leverages Agno's #multi-agent framework for task coordination, and organizes the scraped data #into Google Sheets for further analysis and repurposing.\n",
        "\n",
        "###########################################################################\n",
        "## Components\n",
        "###########################################################################\n",
        "#\n",
        "#1.Twitter Scraping: Retrieve tweets based on a keyword, capturing engagement #metrics and content.\n",
        "#2.MCP Server Integration: Use MCP for message passing between different #components of the system.\n",
        "#3.Agno Multi-Agent Framework: Coordinate multiple agents to handle scraping, #processing, and analysis tasks.\n",
        "#4.Google Sheets Integration: Store the organized data in Google Sheets for #easy access and analysis.\n",
        "#5.Data Analysis and Visualization: Analyze the scraped data to identify trends #and visualize engagement metrics.\n",
        "#\n",
        "###########################################################################\n",
        "\n",
        "# Visualization for What Section\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.text(0.5, 0.5, 'Workflow Components', horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.title('Overview of Workflow Components')\n",
        "plt.show()\n",
        "\n",
        "# Why ?\n",
        "## Efficiency\n",
        "#Manually scrolling through social media for content ideas is inefficient and #time-consuming. This workflow automates the process, reducing it to minutes.\n",
        "\n",
        "## Decentralized Communication\n",
        "#MCP allows for decentralized message passing, ensuring that different #components of the system can communicate effectively without a central point #of failure.\n",
        "\n",
        "## Multi-Agent Coordination\n",
        "#Agno's framework enables the coordination of multiple agents, each specialized #in a specific task, improving the overall efficiency and reliability of the #workflow.\n",
        "\n",
        "## Scalability and Availability\n",
        "#The workflow is designed to handle high-scale data processing and maintain #availability through error handling, rate limiting, and parallel processing.\n",
        "\n",
        "## Data-Driven Insights\n",
        "#By analyzing engagement data, we can identify what content resonates with the #audience, informing better content strategies and decision-making.\n",
        "\n",
        "# Visualization for Why Section\n",
        "reasons = ['Efficiency', 'Decentralized Communication', 'Multi-Agent Coordination', 'Scalability and Availability', 'Data-Driven Insights']\n",
        "importance = [8, 7, 8, 9, 9]  # Hypothetical importance scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(reasons, importance, color=['blue', 'green', 'orange', 'purple', 'red'])\n",
        "plt.xlabel('Reasons')\n",
        "plt.ylabel('Importance (1-10)')\n",
        "plt.title('Importance of Workflow Features')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# How\n",
        "## Environment Setup\n",
        "#We will use Google Colab to set up the environment, installing necessary #libraries like `tweepy`, `google-api-python-client`, `pandas`, `numpy`, #`matplotlib`, `agno`, and `mcp`.\n",
        "\n",
        "## Authentication\n",
        "#Authenticate with Twitter API using `tweepy` and `google-auth` and `google-api-python-client`.\n",
        "\n",
        "## MCP Server Integration\n",
        "#Set up an MCP client to connect to the MCP server and define endpoints for #scraping requests and data processing.\n",
        "\n",
        "## Agno Multi-Agent Framework\n",
        "#Define agents for scraping and processing tasks, create an agent team, and set #up a workflow to coordinate these tasks.\n",
        "\n",
        "## Twitter Scraping\n",
        "#Implement a function to scrape tweets based on a keyword, capturing engagement #metrics and content.\n",
        "\n",
        "## Data Processing and Analysis\n",
        "#Process the scraped data into a pandas DataFrame, sort it by engagement #metrics, and visualize the results.\n",
        "\n",
        "## Google Sheets Integration\n",
        "#Write the processed data to Google Sheets using the Google Sheets API.\n",
        "\n",
        "## Scalability and Availability\n",
        "#Implement rate limiting and parallel processing to ensure the workflow can #handle high-scale data and maintain availability.\n",
        "\n",
        "# Visualization for How Section\n",
        "steps = ['Environment Setup', 'Authentication', 'MCP Integration', 'Agno Framework', 'Twitter Scraping', 'Data Processing', 'Google Sheets', 'Scalability']\n",
        "duration = [2, 1, 3, 4, 5, 3, 2, 4]  # Hypothetical durations in minutes\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(steps, duration, color='skyblue')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.title('Estimated Duration for Each Step')\n",
        "plt.show()\n",
        "\n",
        "# Outcomes\n",
        "## Prototype Completion\n",
        "#A fully functional prototype in Google Colab that demonstrates the entire #workflow from scraping to analysis and storage.\n",
        "\n",
        "## Insights\n",
        "#Detailed insights into trending content and engagement patterns, identified #through data analysis and visualization.\n",
        "\n",
        "## Scalability Blueprint\n",
        "#A clear blueprint for scaling the workflow to production, including #considerations for error handling, rate limiting, and parallel processing.\n",
        "\n",
        "## Production Readiness\n",
        "#The workflow is designed with production deployment in mind, ensuring high #availability and reliability when scaled.\n",
        "\n",
        "## Reusable Code and Documentation\n",
        "#Comprehensive code and documentation that can be easily adapted for production #implementation, facilitating a smooth transition from prototype to production.\n",
        "\n",
        "# Visualization for Outcomes Section\n",
        "outcomes = ['Prototype Completion', 'Insights', 'Scalability Blueprint', 'Production Readiness', 'Reusable Code']\n",
        "impact = [9, 8, 7, 8, 9]  # Hypothetical impact scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.pie(impact, labels=outcomes, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Impact of Outcomes')\n",
        "plt.show()\n",
        "\n",
        "##########################################################################\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "!pip install tweepy google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client pandas numpy matplotlib pyautogen pip-check flake8 isort\n",
        "\n",
        "import tweepy\n",
        "from google.colab import auth\n",
        "from google.colab import userdata\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import autogen\n",
        "from google.auth import default # Import default\n",
        "\n",
        "# Step 2: Authenticate with Twitter and Google APIs\n",
        "print(\"Authenticating with Google...\")\n",
        "auth.authenticate_user()\n",
        "creds, _ = default() # Use default() after authenticate_user()\n",
        "service = build('sheets', 'v4', credentials=creds)\n",
        "print(\"Google authentication successful.\")\n",
        "\n",
        "print(\"\\nAuthenticating with Twitter API...\")\n",
        "# Twitter API Authentication (using input for demonstration as requested)\n",
        "# NOTE: Using input() is less secure than Colab secrets as credentials can appear in output/history.\n",
        "# The recommended secure way is to use Colab's Secrets manager.\n",
        "print(\"🔑 Enter your Twitter API credentials below\")\n",
        "twitter_api_key = input(\"API Key: \")\n",
        "twitter_api_secret = input(\"API Secret: \")\n",
        "twitter_access_token = input(\"Access Token: \")\n",
        "twitter_access_token_secret = input(\"Access Token Secret: \")\n",
        "\n",
        "\n",
        "if not all([twitter_api_key, twitter_api_secret, twitter_access_token, twitter_access_token_secret]):\n",
        "    print(\"⚠️ Warning: Twitter API credentials not provided.\")\n",
        "    print(\"Please enter your credentials when prompted.\")\n",
        "else:\n",
        "    try:\n",
        "        # auth = tweepy.OAuthHandler(twitter_api_key, twitter_api_secret)\n",
        "        # auth.set_access_token(twitter_access_token, twitter_access_token_secret)\n",
        "        # api = tweepy.API(auth) # No longer needed for v2 endpoints\n",
        "        # Verify credentials by making a simple API call using Client\n",
        "        client_v2 = tweepy.Client(\n",
        "            consumer_key=twitter_api_key,\n",
        "            consumer_secret=twitter_api_secret,\n",
        "            access_token=twitter_access_token,\n",
        "            access_token_secret=twitter_access_token_secret\n",
        "        )\n",
        "        # Attempt a simple call to verify credentials\n",
        "        client_v2.get_me() # A simple v2 endpoint call\n",
        "        print(\"✅ Twitter API authentication successful.\")\n",
        "    except tweepy.TweepyException as e:\n",
        "        print(f\"❌ Twitter API authentication failed: {e}\")\n",
        "        print(\"Please check your Twitter API credentials.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Twitter authentication: {e}\")\n",
        "\n",
        "\n",
        "# Step 3: Set Up Autogen Agents (MCP removed)\n",
        "# Autogen Agents - This is a placeholder and will be adapted in the next steps\n",
        "# to integrate with the Twitter scraping functions.\n",
        "# user_proxy = autogen.UserProxyAgent(name=\"user_proxy\", code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"tasks\"})\n",
        "# assistant = autogen.AssistantAgent(name=\"assistant\")\n",
        "\n",
        "\n",
        "# Step 4: Scrape and Process Tweets\n",
        "def scrape_tweets(keyword, count=100):\n",
        "    tweets = []\n",
        "    try:\n",
        "        # Initialize Tweepy Client for v2 endpoints within the function\n",
        "        client = tweepy.Client(\n",
        "            consumer_key=twitter_api_key,\n",
        "            consumer_secret=twitter_api_secret,\n",
        "            access_token=twitter_access_token,\n",
        "            access_token_secret=twitter_access_token_secret\n",
        "        )\n",
        "        # Use client.search_recent_tweets with pagination\n",
        "        for response in tweepy.Paginator(client.search_recent_tweets,\n",
        "                                         query=keyword,\n",
        "                                         tweet_fields=['public_metrics', 'created_at'],\n",
        "                                         max_results=100).flatten(limit=count):\n",
        "            # Safely extract public metrics, defaulting to an empty dictionary if not present\n",
        "            public_metrics = response.data.get('public_metrics', {}) if response.data and hasattr(response.data, 'get') else {}\n",
        "\n",
        "            tweets.append({\n",
        "                'text': response.text,\n",
        "                'id': response.id,\n",
        "                # Construct URL if user exists - Note: user object might not be included by default in v2 search\n",
        "                'url': f\"https://twitter.com/user/status/{response.id}\", # Note: User info is not guaranteed in search_recent_tweets without specifying user_fields\n",
        "                'likes': public_metrics.get('like_count', 0),\n",
        "                'retweets': public_metrics.get('retweet_count', 0),\n",
        "                'replies': public_metrics.get('reply_count', 0),\n",
        "                'views': public_metrics.get('impression_count', 0),\n",
        "                'date': response.created_at\n",
        "            })\n",
        "    except tweepy.TweepyException as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    return tweets\n",
        "\n",
        "def process_data(data):\n",
        "    if not data: # Check if the data list is empty\n",
        "        print(\"No data to process. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame() # Return an empty DataFrame\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    top_tweets = df.sort_values(by='likes', ascending=False).head(10)\n",
        "    return df\n",
        "\n",
        "# Step 5: Integrate with Google Sheets\n",
        "SPREADSHEET_ID = 'YOUR_SPREADSHEET_ID'\n",
        "RANGE_NAME = 'Sheet1!A1'\n",
        "\n",
        "tweets_data = scrape_tweets('AI automation', count=100)\n",
        "df = process_data(tweets_data)\n",
        "\n",
        "# Check if DataFrame is empty before proceeding with Google Sheets and Visualization\n",
        "if not df.empty:\n",
        "    values = [list(df.columns)] + df.values.tolist()\n",
        "    body = {'values': values}\n",
        "    try:\n",
        "        result = service.spreadsheets().values().update(\n",
        "            spreadsheetId=SPREADSHEET_ID, range=RANGE_NAME,\n",
        "            valueInputOption='RAW', body=body).execute()\n",
        "        print(f\"{result.get('updatedCells')} cells updated.\")\n",
        "    except HttpError as error:\n",
        "        print(f\"An error occurred: {error}\")\n",
        "\n",
        "    # Step 6: Analyze and Visualize Data\n",
        "    top_tweets = df.sort_values(by='likes', ascending=False).head(10)\n",
        "    print(\"Top 10 tweets by likes:\")\n",
        "    print(top_tweets[['text', 'likes', 'retweets', 'replies']])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(df['date'], df['likes'], color='blue', alpha=0.6, label='Likes')\n",
        "    plt.bar(df['date'], df['retweets'], color='green', alpha=0.6, label='Retweets')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Engagement Over Time')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No data to write to Google Sheets or visualize.\")\n",
        "\n",
        "\n",
        "# Step 7: Ensure Scalability and Availability\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def rate_limited_scrape(keyword, count=100, delay=15):\n",
        "    tweets = []\n",
        "    for i in range(0, count, 100):\n",
        "        try:\n",
        "            batch = scrape_tweets(keyword, count=100)\n",
        "            tweets.extend(batch)\n",
        "            time.sleep(delay)\n",
        "        except tweepy.TweepyException as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            time.sleep(delay * 2)\n",
        "    return tweets\n",
        "\n",
        "def parallel_scrape(keywords, count_per_keyword=100):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(executor.map(lambda kw: rate_limited_scrape(kw, count_per_keyword), keywords))\n",
        "    return [item for sublist in results for item in sublist]"
      ]
    }
  ]
}